{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":118448,"databundleVersionId":14535669,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nimport kaggle_evaluation.aimo_3_inference_server\nimport pandas as pd\nimport polars as pl\n\n\nclass Model:\n    \"\"\"A dummy model.\"\"\"\n\n    def __init__(self):\n        self._model = None\n\n    def load(self):\n        \"\"\"Simulate model loading.\"\"\"\n        print(\"Loading model...\")\n        # Just return a \"model\" that always answers with 0\n        return lambda problem: 0\n\n    def predict(self, problem: str):\n        # Employ lazy loading: load model on the first model.predict call\n        if self._model is None:\n            self._model = self.load()\n        return self._model(problem)\n\n\nmodel = Model()\n\n\n# Replace this function with your inference code.\n# The function should return a single integer between 0 and 99999, inclusive.\ndef predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame | pd.DataFrame:\n    \"\"\"Make a prediction.\"\"\"\n    # Unpack values\n    id_ = id_.item(0)\n    problem_text: str = problem.item(0)\n    # Make a prediction\n    # The model is loaded on the first call\n    prediction = model.predict(problem_text)\n    return pl.DataFrame({'id': id_, 'answer': prediction})\n\n\ninference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(\n    predict\n)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    # You MUST call this within 15 minutes of the script starting. This is to\n    # ensure a \"fast fail\" in case a bug prevents the inference server from starting.\n    # Do anything that might take a long time (like model loading) in the predict\n    # function, which has no time limit.\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n    )\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:30:56.999188Z","iopub.execute_input":"2025-11-12T18:30:56.999357Z","iopub.status.idle":"2025-11-12T18:31:01.221145Z","shell.execute_reply.started":"2025-11-12T18:30:56.999337Z","shell.execute_reply":"2025-11-12T18:31:01.219701Z"}},"outputs":[],"execution_count":null}]}